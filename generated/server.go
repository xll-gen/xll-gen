// Code generated by xll-gen v0.2.0. DO NOT EDIT.
package generated

import (
	"context"
	"fmt"
	"math/rand"
	"os"
	"strings"
	"sync"
	"time"
	"runtime"
	"runtime/debug"
	"sort"
	"github.com/xll-gen/xll-gen/generated/ipc"
	"github.com/xll-gen/xll-gen/generated/ipc/types"
	"github.com/xll-gen/xll-gen/pkg/log"
	"github.com/xll-gen/shm/go"
	flatbuffers "github.com/google/flatbuffers/go"
)

// Force usage of time and ipc/types to avoid unused import error
var _ = time.Now
var _ = types.Bool{}

const (
	MSG_ACK                  = 2
	MSG_BATCH_ASYNC_RESPONSE = 127
	MSG_CHUNK                = 128
	MSG_SETREFCACHE          = 129
	MSG_CALCULATION_ENDED    = 130
	MSG_CALCULATION_CANCELED = 131
	MSG_USER_START           = 132
)

type chunkBuffer struct {
	data       []byte
	totalSize  int
	received   int
	mutex      sync.Mutex
	lastAccess time.Time
}

type outgoingChunk struct {
	data       []byte
	offset     int
	id         uint64
	msgType    uint32
	lastAccess time.Time
}

// Command Queue
type queuedCommand struct {
	cmdType int // 0: Set, 1: Format
	data    []byte
}
var (
	cmdQueueLock sync.Mutex
	cmdQueue     []queuedCommand
)

// Async Result Buffer
type pendingAsyncResult struct {
	handle  uint64
	valType types.AnyValue // Enum
	val     interface{}    // Go value or struct
	err     string
}

var (
	// Channel for async results (buffer size 1024 to absorb bursts)
	asyncResultQueue = make(chan pendingAsyncResult, 1024)

	// Builder Pool to reduce GC pressure
	builderPool = sync.Pool{
		New: func() interface{} {
			return flatbuffers.NewBuilder(1024)
		},
	}
)

// Optimization Structures
type Cell struct {
	Row int32
	Col int32
}

type mergedRect struct {
	r1, r2, c1, c2 int32
}

type ScalarValue struct {
	Type types.AnyValue
	Num  float64
	Int  int32
	Bool bool
	Str  string
	Err  int16
}

var (
	bufferedSets      = make(map[string]map[Cell]ScalarValue)
	bufferedFormats   = make(map[string]map[Cell]string)
	bufferLock        sync.Mutex
)

func ScheduleSet(r *types.Range, v *types.Any) {
	scalar, ok := toScalar(v)
	if ok {
		sheet := string(r.SheetName())
		bufferLock.Lock()
		if bufferedSets[sheet] == nil {
			bufferedSets[sheet] = make(map[Cell]ScalarValue)
		}

		l := r.RefsLength()
		for i := 0; i < l; i++ {
			var rect types.Rect
			if r.Refs(&rect, i) {
				for row := rect.RowFirst(); row <= rect.RowLast(); row++ {
					for col := rect.ColFirst(); col <= rect.ColLast(); col++ {
						bufferedSets[sheet][Cell{row, col}] = scalar
					}
				}
			}
		}
		bufferLock.Unlock()
		return
	}

	flushBuffers()

	b := flatbuffers.NewBuilder(0)
	rOff := cloneRange(b, r)
	vOff := cloneAny(b, v)

	ipc.SetCommandStart(b)
	ipc.SetCommandAddTarget(b, rOff)
	ipc.SetCommandAddValue(b, vOff)
	root := ipc.SetCommandEnd(b)
	b.Finish(root)

	cmdQueueLock.Lock()
	cmdQueue = append(cmdQueue, queuedCommand{0, b.FinishedBytes()})
	cmdQueueLock.Unlock()
}

func ScheduleFormat(r *types.Range, fmtStr string) {
	sheet := string(r.SheetName())
	bufferLock.Lock()
	if bufferedFormats[sheet] == nil {
		bufferedFormats[sheet] = make(map[Cell]string)
	}

	l := r.RefsLength()
	for i := 0; i < l; i++ {
		var rect types.Rect
		if r.Refs(&rect, i) {
			for row := rect.RowFirst(); row <= rect.RowLast(); row++ {
				for col := rect.ColFirst(); col <= rect.ColLast(); col++ {
					bufferedFormats[sheet][Cell{row, col}] = fmtStr
				}
			}
		}
	}
	bufferLock.Unlock()
}

func flushBuffers() {
	bufferLock.Lock()
	defer bufferLock.Unlock()

	// Process Sets
	for sheet, cells := range bufferedSets {
		byVal := make(map[ScalarValue][]Cell)
		for cell, val := range cells {
			byVal[val] = append(byVal[val], cell)
		}

		for val, cellList := range byVal {
			rects := greedyMesh(cellList)

			// Chunk by 32
			for i := 0; i < len(rects); i += 32 {
				end := i + 32
				if end > len(rects) { end = len(rects) }
				batch := rects[i:end]

				b := flatbuffers.NewBuilder(0)
				sOff := b.CreateString(sheet)

				types.RangeStartRefsVector(b, len(batch))
				for j := len(batch) - 1; j >= 0; j-- {
					types.CreateRect(b, batch[j].r1, batch[j].r2, batch[j].c1, batch[j].c2)
				}
				refsOff := b.EndVector(len(batch))

				types.RangeStart(b)
				types.RangeAddSheetName(b, sOff)
				types.RangeAddRefs(b, refsOff)
				rOff := types.RangeEnd(b)

				vOff := createScalarAny(b, val)

				ipc.SetCommandStart(b)
				ipc.SetCommandAddTarget(b, rOff)
				ipc.SetCommandAddValue(b, vOff)
				uOff := ipc.SetCommandEnd(b)
				b.Finish(uOff)

				cmdQueueLock.Lock()
				cmdQueue = append(cmdQueue, queuedCommand{0, b.FinishedBytes()})
				cmdQueueLock.Unlock()
			}
		}
		delete(bufferedSets, sheet)
	}

	// Process Formats
	for sheet, cells := range bufferedFormats {
		byFmt := make(map[string][]Cell)
		for cell, fmt := range cells {
			byFmt[fmt] = append(byFmt[fmt], cell)
		}

		for fmt, cellList := range byFmt {
			rects := greedyMesh(cellList)

			for i := 0; i < len(rects); i += 32 {
				end := i + 32
				if end > len(rects) { end = len(rects) }
				batch := rects[i:end]

				b := flatbuffers.NewBuilder(0)
				sOff := b.CreateString(sheet)

				types.RangeStartRefsVector(b, len(batch))
				for j := len(batch) - 1; j >= 0; j-- {
					types.CreateRect(b, batch[j].r1, batch[j].r2, batch[j].c1, batch[j].c2)
				}
				refsOff := b.EndVector(len(batch))

				types.RangeStart(b)
				types.RangeAddSheetName(b, sOff)
				types.RangeAddRefs(b, refsOff)
				rOff := types.RangeEnd(b)

				fOff := b.CreateString(fmt)

				ipc.FormatCommandStart(b)
				ipc.FormatCommandAddTarget(b, rOff)
				ipc.FormatCommandAddFormat(b, fOff)
				uOff := ipc.FormatCommandEnd(b)
				b.Finish(uOff)

				cmdQueueLock.Lock()
				cmdQueue = append(cmdQueue, queuedCommand{1, b.FinishedBytes()})
				cmdQueueLock.Unlock()
			}
		}
		delete(bufferedFormats, sheet)
	}
}

func greedyMesh(cells []Cell) []mergedRect {
	if len(cells) == 0 { return nil }

	sort.Slice(cells, func(i, j int) bool {
		if cells[i].Row != cells[j].Row {
			return cells[i].Row < cells[j].Row
		}
		return cells[i].Col < cells[j].Col
	})

	grid := make(map[Cell]bool, len(cells))
	for _, c := range cells {
		grid[c] = true
	}

	var rects []mergedRect
	visited := make(map[Cell]bool, len(cells))

	for _, c := range cells {
		if visited[c] { continue }

		rFirst, cFirst := c.Row, c.Col
		rLast, cLast := rFirst, cFirst

		// Expand Width
		for {
			nextCol := cLast + 1
			if grid[Cell{rFirst, nextCol}] && !visited[Cell{rFirst, nextCol}] {
				cLast = nextCol
			} else {
				break
			}
		}

		// Expand Height
		for {
			nextRow := rLast + 1
			canExpand := true
			for col := cFirst; col <= cLast; col++ {
				if !grid[Cell{nextRow, col}] || visited[Cell{nextRow, col}] {
					canExpand = false
					break
				}
			}
			if canExpand {
				rLast = nextRow
			} else {
				break
			}
		}

		for r := rFirst; r <= rLast; r++ {
			for c := cFirst; c <= cLast; c++ {
				visited[Cell{r, c}] = true
			}
		}

		rects = append(rects, mergedRect{rFirst, rLast, cFirst, cLast})
	}
	return rects
}

func toScalar(v *types.Any) (ScalarValue, bool) {
	if v == nil { return ScalarValue{}, false }
	var tbl flatbuffers.Table
	if !v.Val(&tbl) { return ScalarValue{}, false }

	switch v.ValType() {
	case types.AnyValueInt:
		var t types.Int; t.Init(tbl.Bytes, tbl.Pos)
		return ScalarValue{Type: types.AnyValueInt, Int: t.Val()}, true
	case types.AnyValueNum:
		var t types.Num; t.Init(tbl.Bytes, tbl.Pos)
		return ScalarValue{Type: types.AnyValueNum, Num: t.Val()}, true
	case types.AnyValueBool:
		var t types.Bool; t.Init(tbl.Bytes, tbl.Pos)
		return ScalarValue{Type: types.AnyValueBool, Bool: t.Val()}, true
	case types.AnyValueStr:
		var t types.Str; t.Init(tbl.Bytes, tbl.Pos)
		return ScalarValue{Type: types.AnyValueStr, Str: string(t.Val())}, true
	case types.AnyValueErr:
		var t types.Err; t.Init(tbl.Bytes, tbl.Pos)
		return ScalarValue{Type: types.AnyValueErr, Err: int16(t.Val())}, true
	}
	return ScalarValue{}, false
}

func createScalarAny(b *flatbuffers.Builder, val ScalarValue) flatbuffers.UOffsetT {
	var uOff flatbuffers.UOffsetT
	switch val.Type {
	case types.AnyValueInt:
		types.IntStart(b)
		types.IntAddVal(b, val.Int)
		uOff = types.IntEnd(b)
	case types.AnyValueNum:
		types.NumStart(b)
		types.NumAddVal(b, val.Num)
		uOff = types.NumEnd(b)
	case types.AnyValueBool:
		types.BoolStart(b)
		types.BoolAddVal(b, val.Bool)
		uOff = types.BoolEnd(b)
	case types.AnyValueStr:
		sOff := b.CreateString(val.Str)
		types.StrStart(b)
		types.StrAddVal(b, sOff)
		uOff = types.StrEnd(b)
	case types.AnyValueErr:
		types.ErrStart(b)
		types.ErrAddVal(b, types.XlError(val.Err))
		uOff = types.ErrEnd(b)
	}

	types.AnyStart(b)
	types.AnyAddValType(b, val.Type)
	types.AnyAddVal(b, uOff)
	return types.AnyEnd(b)
}

func flushAsyncBatch(batch []pendingAsyncResult, client *shm.Client) {
	if len(batch) == 0 {
		return
	}

	b := builderPool.Get().(*flatbuffers.Builder)
	b.Reset()
	defer builderPool.Put(b)

	resultOffsets := make([]flatbuffers.UOffsetT, len(batch))

	for i, res := range batch {
		var uOff flatbuffers.UOffsetT
		var errOff flatbuffers.UOffsetT

		if res.err != "" {
			errOff = b.CreateString(res.err)
		} else {
			// Build Union Value
			switch res.valType {
			case types.AnyValueInt:
				types.IntStart(b)
				types.IntAddVal(b, res.val.(int32))
				uOff = types.IntEnd(b)
			case types.AnyValueNum:
				types.NumStart(b)
				types.NumAddVal(b, res.val.(float64))
				uOff = types.NumEnd(b)
			case types.AnyValueBool:
				types.BoolStart(b)
				types.BoolAddVal(b, res.val.(bool))
				uOff = types.BoolEnd(b)
			case types.AnyValueStr:
				sOff := b.CreateString(res.val.(string))
				types.StrStart(b)
				types.StrAddVal(b, sOff)
				uOff = types.StrEnd(b)
			case types.AnyValueNil:
				types.NilStart(b)
				uOff = types.NilEnd(b)
			}
		}

		ipc.AsyncResultStart(b)
		ipc.AsyncResultAddHandle(b, res.handle)
		if errOff > 0 {
			ipc.AsyncResultAddError(b, errOff)
		} else {
			ipc.AsyncResultAddResultType(b, res.valType)
			ipc.AsyncResultAddResult(b, uOff)
		}
		resultOffsets[i] = ipc.AsyncResultEnd(b)
	}

	ipc.BatchAsyncResponseStartResultsVector(b, len(resultOffsets))
	for i := len(resultOffsets) - 1; i >= 0; i-- {
		b.PrependUOffsetT(resultOffsets[i])
	}
	resultsVec := b.EndVector(len(resultOffsets))

	ipc.BatchAsyncResponseStart(b)
	ipc.BatchAsyncResponseAddResults(b, resultsVec)
	root := ipc.BatchAsyncResponseEnd(b)
	b.Finish(root)

	// Send Batch Message
	// We implement a short retry loop to handle transient buffer fullness.
	msgBytes := b.FinishedBytes()
	var err error
	for i := 0; i < 10; i++ {
		if _, err = client.SendGuestCall(msgBytes, MSG_BATCH_ASYNC_RESPONSE); err == nil {
			return
		}
		// Backoff: 5ms, 10ms, ... 2.5s max total wait
		time.Sleep(5 * time.Millisecond * time.Duration(1<<i))
	}

	if err != nil {
		log.Error("Error sending batch async response after retries", "error", err)
	}
}

func Serve(handler XllService) {
	// Initialize Logger
	if err := log.Init("server.log", "debug"); err != nil {
		fmt.Printf("Failed to initialize logger: %v\n", err)
	}

	name := ""
	for _, arg := range os.Args {
		if strings.HasPrefix(arg, "-xll-shm=") {
			name = strings.TrimPrefix(arg, "-xll-shm=")
		}
	}

	client, err := shm.Connect(shm.ClientConfig{ShmName: name})
	if err != nil {
		log.Error("Failed to connect to SHM", "error", err)
		panic(fmt.Errorf("failed to connect to SHM: %w", err))
	}
	defer client.Close()

	// Global Ref Cache
	var refCacheMutex sync.RWMutex
	refCache := make(map[string][]byte)

	// Chunking Cache
	var chunkCacheMutex sync.Mutex
	chunkCache := make(map[uint64]*chunkBuffer)

	// Outgoing Chunks
	var outgoingChunksMutex sync.Mutex
	outgoingChunks := make(map[uint64]*outgoingChunk)

	// Cleanup Ticker
	go func() {
		ticker := time.NewTicker(30 * time.Second)
		for range ticker.C {
			now := time.Now()
			chunkCacheMutex.Lock()
			for id, buf := range chunkCache {
				if now.Sub(buf.lastAccess) > 60*time.Second {
					delete(chunkCache, id)
				}
			}
			chunkCacheMutex.Unlock()

			outgoingChunksMutex.Lock()
			for id, buf := range outgoingChunks {
				if now.Sub(buf.lastAccess) > 60*time.Second {
					delete(outgoingChunks, id)
				}
			}
			outgoingChunksMutex.Unlock()
		}
	}()

	// Reactive Async Flusher
	go func() {
		const maxBatchSize = 256
		batch := make([]pendingAsyncResult, 0, maxBatchSize)

		for {
			// 1. Wait for at least one item (blocking)
			item, ok := <-asyncResultQueue
			if !ok { return } // Channel closed
			batch = append(batch, item)

			// 2. Greedily drain any other available items (non-blocking)
			drain:
			for len(batch) < maxBatchSize {
				select {
				case nextItem, ok := <-asyncResultQueue:
					if !ok {
						flushAsyncBatch(batch, client)
						return
					}
					batch = append(batch, nextItem)
				default:
					// No more items immediately available, break to flush
					break drain
				}
			}

			// 3. Flush the batch
			flushAsyncBatch(batch, client)

			// 4. Reset batch (keep capacity)
			batch = batch[:0]
		}
	}()

	// Configuration




	workerCount := runtime.NumCPU()
	if n := 0; n > 0 {
		workerCount = n
	}

	// Worker Pool
	jobQueue := make(chan func(), workerCount)
	for i := 0; i < workerCount; i++ {
		go func() {
			for job := range jobQueue {
				job()
			}
		}()
	}

	client.Handle(func(req []byte, respBuf []byte, msgType shm.MsgType) (int32, shm.MsgType) {

        var dispatch func(data []byte, respBuf []byte, mType shm.MsgType) (int32, shm.MsgType)
        dispatch = func(data []byte, respBuf []byte, mType shm.MsgType) (int32, shm.MsgType) {
             builder := builderPool.Get().(*flatbuffers.Builder)
             if respBuf != nil && cap(respBuf) > 0 {
                 builder.Bytes = respBuf
             }
             builder.Reset()
             // Safety: Detach SHM buffer before returning to pool to prevent corruption
             defer func() {
                 builder.Bytes = nil
                 builderPool.Put(builder)
             }()

             switch uint32(mType) {
             case MSG_CALCULATION_ENDED:
                refCacheMutex.Lock()
                refCache = make(map[string][]byte)
                refCacheMutex.Unlock()



                builder.Reset()
                respBytes := flushCommands(builder)
                if len(respBytes) > 0 {
                    // Zero-Copy Return (Negative Size)
                    if cap(builder.Bytes) == cap(respBuf) && len(respBytes) <= len(respBuf) {
                        return -int32(len(respBytes)), MSG_CALCULATION_ENDED
                    }

                    if len(respBytes) > len(respBuf) {
                        log.Warn("CalculationEnded response too large", "size", len(respBytes))
                        return 0, 0
                    }
                    copy(respBuf, respBytes)
                    return int32(len(respBytes)), MSG_CALCULATION_ENDED
                }
                return 0, 0

             case MSG_CALCULATION_CANCELED:
                cmdQueueLock.Lock()
                cmdQueue = nil
                cmdQueueLock.Unlock()
                bufferLock.Lock()
                bufferedSets = make(map[string]map[Cell]ScalarValue)
                bufferedFormats = make(map[string]map[Cell]string)
                bufferLock.Unlock()


                return 0, 0

             case MSG_SETREFCACHE:
                reqObj := ipc.GetRootAsSetRefCacheRequest(data, 0)
                key := string(reqObj.Key())
                reqCopy := make([]byte, len(data))
                copy(reqCopy, data)

                refCacheMutex.Lock()
                refCache[key] = reqCopy
                refCacheMutex.Unlock()

                ipc.AckStart(builder)
                ipc.AckAddOk(builder, true)
                root := ipc.AckEnd(builder)
                builder.Finish(root)

                payload := builder.FinishedBytes()
                if cap(builder.Bytes) == cap(respBuf) && len(payload) <= len(respBuf) {
                    return -int32(len(payload)), MSG_ACK
                }
                if len(payload) > len(respBuf) { return 0, 0 }
                copy(respBuf, payload)
                return int32(len(payload)), MSG_ACK

             case MSG_ACK:
                reqObj := ipc.GetRootAsAck(data, 0)
                id := reqObj.Id()
                outgoingChunksMutex.Lock()
                out, exists := outgoingChunks[id]
                if !exists {
                    outgoingChunksMutex.Unlock()
                    return 0, 0
                }
                out.lastAccess = time.Now()

                const chunkSize = 950 * 1024
                remaining := len(out.data) - out.offset
                currentSize := chunkSize
                if remaining < chunkSize { currentSize = remaining }

                if currentSize <= 0 {
                    delete(outgoingChunks, id)
                    outgoingChunksMutex.Unlock()
                    return 0, 0
                }

                builder.Reset()
                dataOff := builder.CreateByteVector(out.data[out.offset : out.offset+currentSize])
                ipc.ChunkStart(builder)
                ipc.ChunkAddId(builder, id)
                ipc.ChunkAddTotalSize(builder, uint32(len(out.data)))
                ipc.ChunkAddOffset(builder, uint32(out.offset))
                ipc.ChunkAddData(builder, dataOff)
                ipc.ChunkAddMsgType(builder, out.msgType)
                root := ipc.ChunkEnd(builder)
                builder.FinishWithFileIdentifier(root, []byte("XCHN"))

                out.offset += currentSize
                if out.offset >= len(out.data) { delete(outgoingChunks, id) }
                outgoingChunksMutex.Unlock()

                payload := builder.FinishedBytes()
                if cap(builder.Bytes) == cap(respBuf) && len(payload) <= len(respBuf) {
                    return -int32(len(payload)), MSG_CHUNK
                }
                if len(payload) > len(respBuf) { return 0, 0 }
                copy(respBuf, payload)
                return int32(len(payload)), MSG_CHUNK

             case MSG_CHUNK:
                reqObj := ipc.GetRootAsChunk(data, 0)
                id := reqObj.Id()
                total := int(reqObj.TotalSize())
                offset := int(reqObj.Offset())
                dataLen := reqObj.DataLength()

                chunkCacheMutex.Lock()
                buf, exists := chunkCache[id]
                if !exists {
                    buf = &chunkBuffer{
                        data:       make([]byte, total),
                        totalSize:  total,
                        lastAccess: time.Now(),
                    }
                    chunkCache[id] = buf
                }
                buf.lastAccess = time.Now()
                chunkCacheMutex.Unlock()

                buf.mutex.Lock()
                if offset + dataLen <= len(buf.data) {
                    copy(buf.data[offset:], reqObj.DataBytes())
                    buf.received += dataLen
                }
                isComplete := buf.received >= buf.totalSize
                buf.mutex.Unlock()

                if isComplete {
                    chunkCacheMutex.Lock()
                    delete(chunkCache, id)
                    chunkCacheMutex.Unlock()

                    payloadMsgType := reqObj.MsgType()
                    return dispatch(buf.data, respBuf, shm.MsgType(payloadMsgType))
                }

                ipc.AckStart(builder)
                ipc.AckAddId(builder, id)
                ipc.AckAddOk(builder, true)
                root := ipc.AckEnd(builder)
                builder.Finish(root)

                payload := builder.FinishedBytes()
                if cap(builder.Bytes) == cap(respBuf) && len(payload) <= len(respBuf) {
                    return -int32(len(payload)), MSG_CHUNK
                }
                if len(payload) > len(respBuf) { return 0, 0 }
                copy(respBuf, payload)
                return int32(len(payload)), MSG_CHUNK



             case 132: // Add

                ctx := context.Background()
                cancel := func() {}



                defer cancel()
                len, respId := handleAdd(ctx, data, respBuf, handler, builder, client, mType, refCache, &refCacheMutex, outgoingChunks, &outgoingChunksMutex)
                return len, respId


             default:
                return 0, 0
             }
        }

        return dispatch(req, respBuf, msgType)
	})

	client.Start()
	client.Wait()
}

// ... handle functions ...

func handleAdd(ctx context.Context, req []byte, respBuf []byte, handler XllService, b *flatbuffers.Builder, client *shm.Client, msgType shm.MsgType, refCache map[string][]byte, refCacheMutex *sync.RWMutex, outgoingChunks map[uint64]*outgoingChunk, outgoingChunksMutex *sync.Mutex) (int32, shm.MsgType) {
	request := ipc.GetRootAsAddRequest(req, 0)
	_ = request

	// Extract args


	arg_a := request.A()



	arg_b := request.B()






	// Sync execution
	var res float64
	var err error

	// Call handler with panic recovery
	func() {
		defer func() {
			if r := recover(); r != nil {
				stack := debug.Stack()
				log.Error("Panic in sync handler Add", "error", r, "stack", string(stack))
				err = fmt.Errorf("panic: %v", r)
			}
		}()
		res, err = handler.Add(ctx, arg_a, arg_b)
	}()

	b.Reset()
	var errOffset flatbuffers.UOffsetT
	if err != nil {
		errOffset = b.CreateString(err.Error())
	}



	ipc.AddResponseStart(b)
	if err != nil {
		ipc.AddResponseAddError(b, errOffset)
	} else {

		ipc.AddResponseAddResult(b, res)

	}
	root := ipc.AddResponseEnd(b)
	b.Finish(root)

	// Zero-Copy Return (Negative Size)
	// cap(b.Bytes) == cap(respBuf) implies it is still the SHM buffer.
	// If so, data is end-aligned and we return negative size.
	payload := b.FinishedBytes()
	if cap(b.Bytes) == cap(respBuf) && len(payload) <= len(respBuf) {
		return -int32(len(payload)), msgType
	}

	if len(payload) > len(respBuf) {
		// Chunking needed
		transferId := uint64(rand.Int63())

		outgoingChunksMutex.Lock()
		outgoingChunks[transferId] = &outgoingChunk{
			data:       make([]byte, len(payload)),
			id:         transferId,
			msgType:    uint32(msgType),
			lastAccess: time.Now(),
		}
		copy(outgoingChunks[transferId].data, payload)

		out := outgoingChunks[transferId]
		const chunkSize = 950 * 1024
		currentSize := chunkSize
		if len(out.data) < chunkSize {
			currentSize = len(out.data)
		}

		b.Reset()
		dataOff := b.CreateByteVector(out.data[0:currentSize])
		ipc.ChunkStart(b)
		ipc.ChunkAddId(b, transferId)
		ipc.ChunkAddTotalSize(b, uint32(len(out.data)))
		ipc.ChunkAddOffset(b, 0)
		ipc.ChunkAddData(b, dataOff)
		ipc.ChunkAddMsgType(b, uint32(msgType))
		root := ipc.ChunkEnd(b)
		b.FinishWithFileIdentifier(root, []byte("XCHN"))

		out.offset = currentSize
		outgoingChunksMutex.Unlock()

		payload = b.FinishedBytes()
		if len(payload) > len(respBuf) {
			return 0, 0 // Fatal: Chunk header overhead made it > 1MB?
		}
		copy(respBuf, payload)
		return int32(len(payload)), MSG_CHUNK
	}
	copy(respBuf, payload)
	return int32(len(payload)), msgType

}




func flushCommands(b *flatbuffers.Builder) []byte {
    flushBuffers()
	cmdQueueLock.Lock()
	defer cmdQueueLock.Unlock()

	if len(cmdQueue) == 0 {
		return nil
	}

	wrappers := make([]flatbuffers.UOffsetT, len(cmdQueue))

	for i, c := range cmdQueue {
		var uOff flatbuffers.UOffsetT
		var uType ipc.Command

		if c.cmdType == 0 {
			cmd := ipc.GetRootAsSetCommand(c.data, 0)
			rOff := cloneRange(b, cmd.Target(nil))
			vOff := cloneAny(b, cmd.Value(nil))

			ipc.SetCommandStart(b)
			ipc.SetCommandAddTarget(b, rOff)
			ipc.SetCommandAddValue(b, vOff)
			uOff = ipc.SetCommandEnd(b)
			uType = ipc.CommandSetCommand
		} else {
			cmd := ipc.GetRootAsFormatCommand(c.data, 0)
			rOff := cloneRange(b, cmd.Target(nil))
			fOff := b.CreateString(string(cmd.Format()))

			ipc.FormatCommandStart(b)
			ipc.FormatCommandAddTarget(b, rOff)
			ipc.FormatCommandAddFormat(b, fOff)
			uOff = ipc.FormatCommandEnd(b)
			uType = ipc.CommandFormatCommand
		}

		ipc.CommandWrapperStart(b)
		ipc.CommandWrapperAddCmdType(b, uType)
		ipc.CommandWrapperAddCmd(b, uOff)
		wrappers[i] = ipc.CommandWrapperEnd(b)
	}

	ipc.CalculationEndedResponseStartCommandsVector(b, len(wrappers))
	for i := len(wrappers) - 1; i >= 0; i-- {
		b.PrependUOffsetT(wrappers[i])
	}
	cmdsOff := b.EndVector(len(wrappers))

	ipc.CalculationEndedResponseStart(b)
	ipc.CalculationEndedResponseAddCommands(b, cmdsOff)
	root := ipc.CalculationEndedResponseEnd(b)
	b.Finish(root)

	cmdQueue = nil
	return b.FinishedBytes()
}

func cloneRange(b *flatbuffers.Builder, r *types.Range) flatbuffers.UOffsetT {
	if r == nil { return 0 }
	s := r.SheetName()
	sOff := b.CreateString(string(s))

	l := r.RefsLength()
	types.RangeStartRefsVector(b, l)
	for i := l - 1; i >= 0; i-- {
		obj := new(types.Rect)
		if r.Refs(obj, i) {
			types.CreateRect(b, obj.RowFirst(), obj.RowLast(), obj.ColFirst(), obj.ColLast())
		}
	}
	refsOff := b.EndVector(l)

	types.RangeStart(b)
	types.RangeAddSheetName(b, sOff)
	types.RangeAddRefs(b, refsOff)
	return types.RangeEnd(b)
}

func cloneAny(b *flatbuffers.Builder, a *types.Any) flatbuffers.UOffsetT {
	if a == nil { return 0 }
	var uOff flatbuffers.UOffsetT
	t := a.ValType()

	var tbl flatbuffers.Table
	if a.Val(&tbl) {
		switch t {
		case types.AnyValueNum:
			var val types.Num
			val.Init(tbl.Bytes, tbl.Pos)
			types.NumStart(b)
			types.NumAddVal(b, val.Val())
			uOff = types.NumEnd(b)
		case types.AnyValueInt:
			var val types.Int
			val.Init(tbl.Bytes, tbl.Pos)
			types.IntStart(b)
			types.IntAddVal(b, val.Val())
			uOff = types.IntEnd(b)
		case types.AnyValueBool:
			var val types.Bool
			val.Init(tbl.Bytes, tbl.Pos)
			types.BoolStart(b)
			types.BoolAddVal(b, val.Val())
			uOff = types.BoolEnd(b)
		case types.AnyValueStr:
			var val types.Str
			val.Init(tbl.Bytes, tbl.Pos)
			sOff := b.CreateString(string(val.Val()))
			types.StrStart(b)
			types.StrAddVal(b, sOff)
			uOff = types.StrEnd(b)
		case types.AnyValueErr:
			var val types.Err
			val.Init(tbl.Bytes, tbl.Pos)
			types.ErrStart(b)
			types.ErrAddVal(b, val.Val())
			uOff = types.ErrEnd(b)
		case types.AnyValueNumGrid:
			var val types.NumGrid
			val.Init(tbl.Bytes, tbl.Pos)
			l := val.DataLength()
			types.NumGridStartDataVector(b, l)
			for i := l - 1; i >= 0; i-- {
				b.PrependFloat64(val.Data(i))
			}
			dataOff := b.EndVector(l)

			types.NumGridStart(b)
			types.NumGridAddRows(b, val.Rows())
			types.NumGridAddCols(b, val.Cols())
			types.NumGridAddData(b, dataOff)
			uOff = types.NumGridEnd(b)
		case types.AnyValueGrid:
			var val types.Grid
			val.Init(tbl.Bytes, tbl.Pos)
			uOff = cloneGrid(b, &val)
		case types.AnyValueRange:
			var val types.Range
			val.Init(tbl.Bytes, tbl.Pos)
			uOff = cloneRange(b, &val)
		case types.AnyValueRefCache:
			var val types.RefCache
			val.Init(tbl.Bytes, tbl.Pos)
			kOff := b.CreateString(string(val.Key()))
			types.RefCacheStart(b)
			types.RefCacheAddKey(b, kOff)
			uOff = types.RefCacheEnd(b)
		case types.AnyValueAsyncHandle:
			var val types.AsyncHandle
			val.Init(tbl.Bytes, tbl.Pos)
			types.AsyncHandleStart(b)
			types.AsyncHandleAddVal(b, val.Val())
			uOff = types.AsyncHandleEnd(b)
		default:
			// Nil
			types.NilStart(b)
			uOff = types.NilEnd(b)
		}
	} else {
		types.NilStart(b)
		uOff = types.NilEnd(b)
	}

	types.AnyStart(b)
	types.AnyAddValType(b, t)
	types.AnyAddVal(b, uOff)
	return types.AnyEnd(b)
}

func cloneGrid(b *flatbuffers.Builder, g *types.Grid) flatbuffers.UOffsetT {
	if g == nil {
		return 0
	}
	l := g.DataLength()
	offsets := make([]flatbuffers.UOffsetT, l)
	for i := 0; i < l; i++ {
		var s types.Scalar
		if g.Data(&s, i) {
			offsets[i] = cloneScalar(b, &s)
		}
	}

	types.GridStartDataVector(b, l)
	for i := l - 1; i >= 0; i-- {
		b.PrependUOffsetT(offsets[i])
	}
	dataOff := b.EndVector(l)

	types.GridStart(b)
	types.GridAddRows(b, g.Rows())
	types.GridAddCols(b, g.Cols())
	types.GridAddData(b, dataOff)
	return types.GridEnd(b)
}

func cloneScalar(b *flatbuffers.Builder, s *types.Scalar) flatbuffers.UOffsetT {
	if s == nil {
		return 0
	}

	var uOff flatbuffers.UOffsetT
	t := s.ValType()

	var tbl flatbuffers.Table
	if s.Val(&tbl) {
		switch t {
		case types.ScalarValueInt:
			var val types.Int
			val.Init(tbl.Bytes, tbl.Pos)
			types.IntStart(b)
			types.IntAddVal(b, val.Val())
			uOff = types.IntEnd(b)
		case types.ScalarValueNum:
			var val types.Num
			val.Init(tbl.Bytes, tbl.Pos)
			types.NumStart(b)
			types.NumAddVal(b, val.Val())
			uOff = types.NumEnd(b)
		case types.ScalarValueBool:
			var val types.Bool
			val.Init(tbl.Bytes, tbl.Pos)
			types.BoolStart(b)
			types.BoolAddVal(b, val.Val())
			uOff = types.BoolEnd(b)
		case types.ScalarValueStr:
			var val types.Str
			val.Init(tbl.Bytes, tbl.Pos)
			sOff := b.CreateString(string(val.Val()))
			types.StrStart(b)
			types.StrAddVal(b, sOff)
			uOff = types.StrEnd(b)
		case types.ScalarValueErr:
			var val types.Err
			val.Init(tbl.Bytes, tbl.Pos)
			types.ErrStart(b)
			types.ErrAddVal(b, val.Val())
			uOff = types.ErrEnd(b)
		case types.ScalarValueAsyncHandle:
			var val types.AsyncHandle
			val.Init(tbl.Bytes, tbl.Pos)
			types.AsyncHandleStart(b)
			types.AsyncHandleAddVal(b, val.Val())
			uOff = types.AsyncHandleEnd(b)
		default:
			types.NilStart(b)
			uOff = types.NilEnd(b)
		}
	} else {
		types.NilStart(b)
		uOff = types.NilEnd(b)
	}

	types.ScalarStart(b)
	types.ScalarAddValType(b, t)
	types.ScalarAddVal(b, uOff)
	return types.ScalarEnd(b)
}
